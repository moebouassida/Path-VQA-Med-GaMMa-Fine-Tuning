# Model & dataset
pretrained_model: "unsloth/medgemma-4b-it-unsloth-bnb-4bit"

# Training
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
num_train_epochs: 2
learning_rate: 2e-4
warmup_steps: 10
max_grad_norm: 0.3
logging_steps: 10
save_strategy: "steps"
optim: "adamw_torch_fused"
weight_decay: 0.01
lr_scheduler_type: "cosine"
max_seq_length: 2048

# LoRA
lora_r: 16
lora_alpha: 16
lora_dropout: 0

# General
output_dir: "outputs"
resize: 512
seed: 3407
