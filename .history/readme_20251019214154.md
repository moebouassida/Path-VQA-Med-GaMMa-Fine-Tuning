# 🧬 Path-VQA Med-GaMMa Fine-Tuning

** Fine-tune Med-GaMMa on an enhanced version of the Path-VQA dataset, where answers are more pathologically detailed. Deploy via FastAPI with GPU Docker support and MLflow tracking.**

🚧 ** Live demo is currently in progress! **

## ✅ Features
📷 Upload pathology images (via URL) and ask clinical questions

🩺 Predict pathologically accurate answers using Med-GaMMa fine-tuned on enhanced Path-VQA

⚡ GPU acceleration supported (CUDA + Docker)

📊 MLflow integration for tracking training metrics, hyperparameters, and model checkpoints

📝 Interactive API documentation via Swagger UI (/docs)

🧱 Modular, clean project structure for easy extension

## 📁 Project Structure

| File / Folder | Description |

| app/main.py | FastAPI app + inference logic |

| scripts/train.py | Fine-tuning script using LoRA + SFTTrainer |

| scripts/data_preprocessing.py | Converts Path-VQA dataset to conversation format |

| configs/config.yaml | Hyperparameters and training configuration |

| outputs/ | Fine-tuned Med-GaMMa model adapters and checkpoints |

| mlruns/ | MLflow experiment tracking folder |

| Dockerfile | GPU-enabled Docker image |

| requirements.txt | Python dependencies |

| README.md | Project documentation |

## 🚀 Setup & Run

Clone the repository

No local dataset needed — Path-VQA is downloaded directly from Hugging Face.

Run Locally (without Docker)

pip install -r requirements.txt
python scripts/train.py
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

Run with Docker (GPU Recommended)

Requires NVIDIA GPU and NVIDIA Container Toolkit:

docker build -t pathvqa-medgemma-gpu .
docker run --gpus all -p 8000:8000 pathvqa-medgemma-gpu

## 📤 API Endpoints

| Method | Endpoint | Description |
| POST | /predict | Upload image URL + question → returns pathologically detailed answer |
| GET | /health | Health check | 
| GET | /docs | Interactive Swagger UI |

Example request:

curl -X POST "http://localhost:8000/predict" \
-H "Content-Type: application/json" \
-d '{
    "image_url": "https://example.com/pathology_image.jpg",
    "question": "What type of tissue is shown?"
}'

Response:

{
  "answer": "This is glandular epithelium tissue with high stromal content..."
}

## 📊 MLflow Integration

Metrics, hyperparameters, and trained models are logged to ./mlruns.

Start MLflow UI:

mlflow ui --backend-store-uri ./mlruns
# Open in browser: http://localhost:5000

Tracked items:

✅ Training loss & validation metrics✅ Hyperparameters (LR, batch size, epochs)✅ Saved LoRA adapters / model checkpoints

## ⚙ Technologies Used

| Component | Technology |

| Backend | API FastAPI |

| Vision-Language Model | Med-GaMMa |

| Fine-Tuning | LoRA + SFTTrainer (TRL) |

| Experiment | Tracking MLflow |

| Containerization | Docker + NVIDIA Runtime |

| Dataset | Enhanced Path-VQA (answers pathologically detailed) |

| Input | Image + Question (JSON) |

| Output | Text answer (JSON) |


## 🖼 Project Overview Diagram

Path-VQA Enhanced Dataset
        │
        ▼
Data Preprocessing
        │
        ▼
LoRA Fine-Tuning (Med-GaMMa)
        │
        ▼
Saved Adapters / Checkpoints
        │
        ▼
FastAPI Inference API
        │
        ▼
User Input: Image + Question
        │
        ▼
Model Output: Pathologically Detailed Answer

## 🔄 Roadmap

✅ GPU-enabled Docker image
✅ MLflow integration
📊 Web dashboard for predictions & visualizations (in progress)
☁  Cloud deployment (AWS)

